{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Required Components"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras.layers as layers\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from keras.applications.resnet import ResNet50, ResNet152\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.efficientnet_v2 import EfficientNetV2S\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some Preparations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline # Jupyter Magic Function for displaying plotting command under the Jupyter cell\n",
    "np.random.seed(2017) # Setting seed for numpy random generator in order make the model more reproducible"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This function prepares train, validation and evaluation data.\n",
    "# Evaluation data is part of the train set that is not passed to model.fit() so that two models\n",
    "# can be tested with different evaluation data which they have not seen during training at all.\n",
    "\n",
    "def get_data(random_state=30, eval_set=True):\n",
    "    # Reading Train and Test Images along with Train Labels and Saving them to Disk as Numpy Arrays for future use\n",
    "    def prepare_data(train_len=50000,\n",
    "                 test_len=10000,\n",
    "                 x_train_dir=\"data/train\",\n",
    "                 x_test_dir=\"data/test\",\n",
    "                 y_trains=\"data/train_labels.csv\"):\n",
    "\n",
    "        tf_image = tf.keras.preprocessing.image\n",
    "\n",
    "        x_test = []\n",
    "        for i in range(test_len):\n",
    "            image = tf_image.load_img(f\"{x_test_dir}/{i}.jpg\")\n",
    "            image_array = tf_image.img_to_array(image)\n",
    "            x_test.append(image_array)\n",
    "        np.save(\"data/x_test.npy\", np.array(x_test))\n",
    "\n",
    "        x_train = []\n",
    "        for i in range(train_len):\n",
    "            image = tf_image.load_img(f\"{x_train_dir}/{i}.jpg\")\n",
    "            image_array = tf_image.img_to_array(image)\n",
    "            x_train.append(image_array)\n",
    "        np.save(\"data/x_train.npy\", np.array(x_train))\n",
    "\n",
    "        df = pd.read_csv(y_trains)\n",
    "        labels = df[\"label\"].to_numpy()\n",
    "        labels.resize((len(labels), 1))\n",
    "        np.save(\"data/y_train.npy\", labels)\n",
    "\n",
    "    prepare_data()\n",
    "    x_train_orig = np.load(\"data/x_train.npy\")\n",
    "    y_train_orig = np.load(\"data/y_train.npy\")\n",
    "\n",
    "    x_eval, y_eval = None, None\n",
    "\n",
    "    x_train_orig = x_train_orig.astype('float32')\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_orig, y_train_orig, test_size=0.2, random_state=random_state)\n",
    "    if eval_set:\n",
    "        x_train, x_eval, y_train, y_eval = train_test_split(x_train, y_train, test_size=0.15, random_state=random_state)\n",
    "        y_eval = np_utils.to_categorical(y_eval, num_classes)\n",
    "\n",
    "    # convert y_train and y_val (and possibly y_eval) to one hot encoding\n",
    "    y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "    y_val = np_utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "    return x_train, x_val, x_eval, y_train, y_val, y_eval"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting Train, Validation, and Evaluation Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "x_train, x_val, x_eval, y_train, y_val, y_eval = get_data(eval_set=False)\n",
    "\n",
    "print('Train Data Shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'Train Samples')\n",
    "print(x_val.shape[0], 'Validation Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuring Preprocessing Layers For The Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalizer_layer = layers.Normalization(mean=0.5, variance=0.25) # Adding Normalization Layer to the Model\n",
    "\n",
    "# Adding Data Augmentation layers to the model. Except Rescaling layer, the rest are active only during training phase.\n",
    "# The value of these layers may change for different models\n",
    "augmentation_layers = Sequential([\n",
    "    layers.Resizing(36, 36),\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomCrop(32, 32),\n",
    "    layers.RandomZoom(height_factor=(-0.2, 0.2), fill_mode=\"constant\", fill_value=1),\n",
    "    layers.RandomRotation(0.015, fill_mode=\"constant\", fill_value=1),\n",
    "    layers.RandomContrast(0.3),\n",
    "    layers.RandomTranslation(0.4, 0.4),\n",
    "    layers.Rescaling(1./255),\n",
    "])\n",
    "\n",
    "# Bundling Together Augmentation and Normalization Layers\n",
    "preprocessing_layers = Sequential([\n",
    "    augmentation_layers,\n",
    "    normalizer_layer\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining A class For Managing and Configuring Keras Applications"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This class configures and returns desired Keras Application without loading any pretrained weights\n",
    "# based on the name of the application\n",
    "class ModelsRepo:\n",
    "    @staticmethod\n",
    "    def getResnet50(prev_layer):\n",
    "        base = ResNet50(weights=None, input_shape=input_shape, include_top=False)(prev_layer)\n",
    "        avg_pool = GlobalAveragePooling2D()(base)\n",
    "        return avg_pool\n",
    "\n",
    "    @staticmethod\n",
    "    def getResnet152(prev_layer):\n",
    "        base = ResNet152(weights=None, input_shape=input_shape, include_top=False)(prev_layer)\n",
    "        avg_pool = GlobalAveragePooling2D()(base)\n",
    "        return avg_pool\n",
    "\n",
    "    @staticmethod\n",
    "    def getDenseNet121(prev_layer):\n",
    "        base = DenseNet121(weights=None, input_shape=input_shape, include_top=False)(prev_layer)\n",
    "        avg_pool = GlobalAveragePooling2D()(base)\n",
    "        return avg_pool\n",
    "\n",
    "    @staticmethod\n",
    "    def getVGG16(prev_layer):\n",
    "        base = VGG16(weights=None, input_shape=input_shape, include_top=False)(prev_layer)\n",
    "        flat = Flatten()(base)\n",
    "        fc1 = Dense(4096, activation=\"relu\")(flat)\n",
    "        fc2 = Dense(4096, activation=\"relu\")(fc1)\n",
    "        return fc2\n",
    "\n",
    "    @staticmethod\n",
    "    def getVGG19(prev_layer):\n",
    "        base = VGG19(weights=None, input_shape=input_shape, include_top=False)(prev_layer)\n",
    "        flat = Flatten()(base)\n",
    "        fc1 = Dense(4096, activation=\"relu\")(flat)\n",
    "        fc2 = Dense(4096, activation=\"relu\")(fc1)\n",
    "        return fc2\n",
    "\n",
    "    @staticmethod\n",
    "    def getEfficientNetV2S(prev_layer):\n",
    "        base = EfficientNetV2S(weights=None, input_shape=input_shape, include_top=False)(prev_layer)\n",
    "        avg_pool = GlobalAveragePooling2D()(base)\n",
    "        dropout = Dropout(0.2)(avg_pool)\n",
    "        return dropout\n",
    "\n",
    "    @staticmethod\n",
    "    def getModel(model_name, prev_layer):\n",
    "        if model_name == \"resnet50\":\n",
    "            return ModelsRepo.getResnet50(prev_layer)\n",
    "        if model_name == \"resnet152\":\n",
    "            return ModelsRepo.getResnet152(prev_layer)\n",
    "        if model_name == \"densenet121\":\n",
    "            return ModelsRepo.getDenseNet121(prev_layer)\n",
    "        if model_name == \"vgg16\":\n",
    "            return ModelsRepo.getVGG16(prev_layer)\n",
    "        if model_name == \"vgg19\":\n",
    "            return ModelsRepo.getVGG19(prev_layer)\n",
    "        if model_name == \"efficientnetv2s\":\n",
    "            return ModelsRepo.getEfficientNetV2S(prev_layer)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuring Model's Name and Directory For Saving and Loading The Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_names = [\"resnet50\", \"resnet152\", \"densenet121\", \"vgg16\", \"vgg19\", \"efficientnetv2s\"]\n",
    "model_name = \"densenet121\"\n",
    "fileaddr = f\"{model_name}\" # File address used for saving and loading h5 models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuring Callback Function For The model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configuring Early Stopping callback function in order to prevent over-fitting\n",
    "es = EarlyStopping(mode=\"min\", patience=10)\n",
    "\n",
    " # Model Checkpoints to save the model with lowest val_loss during training\n",
    "modelCheckpoint = ModelCheckpoint(f\"{fileaddr}_best.h5\", monitor=\"val_loss\", mode=\"min\", save_best_only=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constructing The Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_shape=x_train.shape[1:]\n",
    "\n",
    "inp = Input(shape=input_shape)\n",
    "preprocess = preprocessing_layers(inp) # Getting the preprocessing layers\n",
    "base = ModelsRepo.getModel(model_name, preprocess) # Getting the desired model as base model\n",
    "out = Dense(num_classes, activation='softmax')(base) # Adding our own output layer according to our classification task\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuring Model With Proper Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuring learning rate of the model. This value is changed both for different models and while\n",
    "# training the same model.\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "epochs = 10 # Configuring the number of epochs for training. This value is changed iteratively during training.\n",
    "\n",
    "batch_size = 128 # Configuring batch size. This value is changed iteratively during training.\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam_v2.Adam(learning_rate=learning_rate),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training The Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tick = time.time()\n",
    "\n",
    "# Fitting the model on train data and saving the history in order to plot and monitor the model during training\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[es, modelCheckpoint]\n",
    "                    )\n",
    "\n",
    "# Plotting Accuracy and Loss of model during training. Used for configuring further batch size, learning rate, and\n",
    "# identifying over-fitting\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Elapsed Time: {time.time() - tick}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving and Loading The Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(f'{fileaddr}.h5') # Saving the current model. Note that this isn't necessarily the best model\n",
    "best_model = keras.models.load_model(f\"{fileaddr}_best.h5\") # Loading the best model of current Keras application"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting Test Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_test = np.load(\"data/x_test.npy\") # Loading Test Data\n",
    "\n",
    "prediction = best_model.predict(x_test)\n",
    "prediction = np.argmax(prediction, axis=1) # Converting one-hot encoding to an array of labels\n",
    "\n",
    "# Creating CSV file for predictions\n",
    "pred_df = pd.DataFrame(columns=[\"id\", \"label\"])\n",
    "pred_df[\"id\"] = np.arange(len(x_test))\n",
    "pred_df[\"label\"] = prediction\n",
    "pred_df.to_csv(\"predictions.csv\", index=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
